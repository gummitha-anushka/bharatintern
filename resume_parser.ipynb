{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOxwk3XwMfXp",
        "outputId": "01d7f513-7ac0-48d0-fadd-11d60e52675c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gummitha Anushka\n",
            "gummithaanushkareddy@gmail.com\n",
            "{'java', 'html', 'c', 'css', 'DBMS', 'leadership', 'communication', 'python'}\n"
          ]
        }
      ],
      "source": [
        "#!pip install PyPDF2\n",
        "#!pip install pdfminer.six\n",
        "import nltk\n",
        "import PyPDF2\n",
        "import nltk\n",
        "import requests\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_skills(input_text):\n",
        "    skills = [\n",
        "        'python', 'java', 'c++', 'javascript', 'html', 'css', 'sql', 'machine learning',\n",
        "        'data analysis', 'data visualization', 'statistics', 'project management','DBMS','c',\n",
        "        'Microsoft Office','OS','leadership','communication','AWS', 'Azure', 'Google Cloud',\n",
        "        'React','Node JS','Django','PHP','Communication skills','Teamwork','collaboration','Problem-solving',\n",
        "        'critical thinking','Time management','organization','Adaptability','flexibility'\n",
        "    ]\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
        "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
        "    filtered_tokens = [w.lower() for w in filtered_tokens]\n",
        "    found_skills = set()\n",
        "    for skill in skills:\n",
        "        if skill.lower() in filtered_tokens:\n",
        "            found_skills.add(skill)\n",
        "    return found_skills\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        " \n",
        "def extract_names(txt):\n",
        "    person_names = []\n",
        "    for sent in nltk.sent_tokenize(txt):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "                person_names.append(\n",
        "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
        "                )\n",
        " \n",
        "    return person_names\n",
        "\n",
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        " \n",
        "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
        " \n",
        "def extract_emails(resume_text):\n",
        "    return re.findall(EMAIL_REG, resume_text)\n",
        " \n",
        "if __name__ == '__main__':\n",
        "    pdf_path = \"/content/Anushka_Gummitha_resume.pdf\"\n",
        "    pdf_text = extract_text_from_pdf(pdf_path)\n",
        "    names = extract_names(pdf_text)\n",
        "    if names:\n",
        "        print(names[1]) \n",
        "    emails = extract_emails(pdf_text)\n",
        "    if emails:\n",
        "        print(emails[0]) \n",
        "    skills = extract_skills(pdf_text)\n",
        "    print(skills)\n"
      ]
    }
  ]
}
